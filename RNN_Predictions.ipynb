{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "182a3810-aab7-4d65-8ff7-0ad50183d769",
    "_uuid": "c8c2cca5ed3dcf9ff50ba2da4c9db56329bfc2cb"
   },
   "source": [
    "# RNN Stock Price Forecasting\n",
    "\n",
    "This notebook demonstrates a complete pipeline for one-step-ahead stock price prediction using recurrent neural networks built with TensorFlow/Keras.\n",
    "\n",
    "**Pipeline overview**\n",
    "\n",
    "1. Setup and imports  \n",
    "2. Load and explore the NYSE price dataset  \n",
    "3. Prepare normalized rolling windows for training/validation/testing  \n",
    "4. Train a stacked GRU model  \n",
    "5. Evaluate predictions with plots and directional-accuracy metrics\n",
    "\n",
    "Dataset: Kaggle \"New York Stock Exchange\" (`prices-split-adjusted.csv`). The code assumes the CSV is extracted under `./prices-split-adjusted.csv/`.\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1426ca5e-ed0c-4766-ac74-cd1dd38c680c",
    "_uuid": "8ec772a02eb88c3446fe0f689b2e1ee8ec9deb6b"
   },
   "source": [
    "# 1. Setup and Imports\n",
    "\n",
    "Configure libraries, random seeds, and global settings used throughout the notebook.\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a142d3f-96b5-4378-bb01-903367721e08",
    "_uuid": "78e6c0015907947ff5fade7ad76317633d27177a",
    "collapsed": true,
    "trusted": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "VALID_SPLIT_PCT = 10\n",
    "TEST_SPLIT_PCT = 10\n",
    "FEATURES = ['open', 'high', 'low', 'close']  # Core price features\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Sample files: {os.listdir(os.getcwd())[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5416ce8a-24ff-4903-a09b-b2941eae71d9",
    "_uuid": "9422490f417bc1fcac8e6a0a392faa7451173dac"
   },
   "source": [
    "# 2. Load Data\n",
    "\n",
    "Load daily prices from the Kaggle NYSE dataset, inspect the available tickers, and select one symbol to model. Adjust `ticker` as needed.\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60dbc696-dda8-476e-8227-010a79df3aa2",
    "_uuid": "aef1074dd4f86a9fe5a380c83994123ce56cce9d",
    "collapsed": true,
    "trusted": false
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"prices-split-adjusted.csv/prices-split-adjusted.csv\"\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected to find {DATA_PATH}. Please download and extract the Kaggle dataset.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Rows: {len(df):,}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "symbols = sorted(df['symbol'].unique())\n",
    "print(f\"Number of tickers: {len(symbols)}\")\n",
    "print(f\"Sample tickers: {symbols[:10]}\")\n",
    "\n",
    "ticker = \"EQIX\"\n",
    "\n",
    "if ticker not in symbols:\n",
    "    raise ValueError(f\"Ticker {ticker} not found in dataset.\")\n",
    "\n",
    "stock = (\n",
    "    df.loc[df.symbol == ticker, FEATURES]\n",
    "      .sort_index()\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "stock_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(stock),\n",
    "    columns=FEATURES\n",
    ")\n",
    "\n",
    "stock_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73ad0def-f162-42b9-a1d6-aac45be27e81",
    "_uuid": "777e047511205f929abf0867c7c8a2edee2531ca"
   },
   "source": [
    "# 3. Prepare Data\n",
    "\n",
    "Create normalized rolling windows to build supervised sequences and split them chronologically into training, validation, and test sets.\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e577ab98-344e-4a8e-88c2-09a7d45834b9",
    "_uuid": "3333a7ef79ad756a4b0190e8aef2ed8b3624d7d8",
    "collapsed": true,
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def build_windows(values: np.ndarray, seq_len: int):\n",
    "    \"\"\"Create rolling windows of length `seq_len` from a [time, features] array.\"\"\"\n",
    "    if len(values) <= seq_len:\n",
    "        raise ValueError(\"Not enough observations to build training windows.\")\n",
    "    windows = np.array([values[i:i + seq_len] for i in range(len(values) - seq_len)], dtype=np.float32)\n",
    "    x = windows[:, :-1, :]\n",
    "    y = windows[:, -1, :]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def split_data(x: np.ndarray, y: np.ndarray, valid_pct: int, test_pct: int):\n",
    "    \"\"\"Split the sequences chronologically into train/validation/test subsets.\"\"\"\n",
    "    total = x.shape[0]\n",
    "    valid_len = int(round(valid_pct / 100 * total))\n",
    "    test_len = int(round(test_pct / 100 * total))\n",
    "    train_len = total - valid_len - test_len\n",
    "    if train_len <= 0:\n",
    "        raise ValueError(\"Split configuration leaves no data for training.\")\n",
    "    return (\n",
    "        x[:train_len], y[:train_len],\n",
    "        x[train_len:train_len + valid_len], y[train_len:train_len + valid_len],\n",
    "        x[train_len + valid_len:], y[train_len + valid_len:]\n",
    "    )\n",
    "\n",
    "\n",
    "seq_len = 20\n",
    "\n",
    "x_all, y_all = build_windows(stock_scaled.values, seq_len)\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = split_data(\n",
    "    x_all, y_all, VALID_SPLIT_PCT, TEST_SPLIT_PCT\n",
    ")\n",
    "\n",
    "for name, array in {\n",
    "    \"x_train\": x_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"x_valid\": x_valid,\n",
    "    \"y_valid\": y_valid,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_test\": y_test,\n",
    "}.items():\n",
    "    print(f\"{name}.shape = {array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "27e07a3b-89ea-48cc-bd86-023785634d1e",
    "_uuid": "c5ce171e4160f5e0175e4e5ab8e242f181262440",
    "collapsed": true,
    "scrolled": true,
    "trusted": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for feature, color in zip(FEATURES, [\"red\", \"green\", \"blue\", \"black\"]):\n",
    "    plt.plot(stock_scaled[feature].values, color=color, label=feature)\n",
    "\n",
    "plt.title(f\"{ticker} - normalized feature history\")\n",
    "plt.xlabel(\"Trading days\")\n",
    "plt.ylabel(\"Scaled price\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4375362b-7a7c-4572-947c-91f46c4fd77c",
    "_uuid": "a495f69f3a265f419cd5aa0a4119a8b0392365fd"
   },
   "source": [
    "# 4. Model Training\n",
    "\n",
    "Train a two-layer GRU network to predict the next day's normalized prices, using early stopping on the validation loss to avoid overfitting.\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dde96de3-fef3-45cf-959c-22171a28a3b6",
    "_uuid": "b5015613883c52563da5f8e5be8d6adeb274e420",
    "collapsed": true,
    "trusted": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(\n",
    "        units=200,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=True,\n",
    "        input_shape=(seq_len - 1, len(FEATURES))\n",
    "    ),\n",
    "    tf.keras.layers.GRU(\n",
    "        units=200,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=False\n",
    "    ),\n",
    "    tf.keras.layers.Dense(len(FEATURES))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=50,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_valid_pred = model.predict(x_valid)\n",
    "y_test_pred = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9035de79-0f5c-4b6b-bc86-0d8d4e2f392e",
    "_uuid": "a88583a9effb6f1f84e13b8853adca62578c2992"
   },
   "source": [
    "# 5. Evaluate Predictions\n",
    "\n",
    "Visualise forecasts against actual normalized prices and report mean squared error, mean absolute error, and directional accuracy (close minus open sign).\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1bb898b9-943e-4e6f-b43d-8bdb6331cc4e",
    "_uuid": "6fa5eb60c8e8a5d3542547629c3e0df14e5c33a3",
    "collapsed": true,
    "trusted": false
   },
   "outputs": [],
   "source": [
    "target_feature_idx = 1  # Close price\n",
    "\n",
    "def directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    if len(y_true) == 0:\n",
    "        return float(\"nan\")\n",
    "    true_direction = np.sign(y_true[:, 1] - y_true[:, 0])\n",
    "    pred_direction = np.sign(y_pred[:, 1] - y_pred[:, 0])\n",
    "    return np.mean(true_direction == pred_direction)\n",
    "\n",
    "splits = [\n",
    "    (\"Train\", y_train, y_train_pred),\n",
    "    (\"Validation\", y_valid, y_valid_pred),\n",
    "    (\"Test\", y_test, y_test_pred),\n",
    "]\n",
    "\n",
    "timelines = {\n",
    "    \"Train\": (0, len(y_train)),\n",
    "    \"Validation\": (len(y_train), len(y_train) + len(y_valid)),\n",
    "    \"Test\": (len(y_train) + len(y_valid), len(y_train) + len(y_valid) + len(y_test)),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for split_name, y_true, _ in splits:\n",
    "    start, end = timelines[split_name]\n",
    "    if len(y_true) == 0:\n",
    "        continue\n",
    "    plt.plot(\n",
    "        range(start, end),\n",
    "        y_true[:, target_feature_idx],\n",
    "        label=f\"{split_name.lower()} target\"\n",
    "    )\n",
    "\n",
    "for split_name, _, y_pred in splits:\n",
    "    start, end = timelines[split_name]\n",
    "    if len(y_pred) == 0:\n",
    "        continue\n",
    "    plt.plot(\n",
    "        range(start, end),\n",
    "        y_pred[:, target_feature_idx],\n",
    "        linestyle=\"--\",\n",
    "        label=f\"{split_name.lower()} prediction\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Normalized close price: actual vs. predicted\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"Scaled price\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(y_test) > 0:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    start, end = timelines[\"Test\"]\n",
    "    plt.plot(range(start, end), y_test[:, target_feature_idx], label=\"test target\", color=\"black\")\n",
    "    plt.plot(range(start, end), y_test_pred[:, target_feature_idx], label=\"test prediction\", color=\"green\")\n",
    "    plt.title(\"Test window: normalized close price\")\n",
    "    plt.xlabel(\"Time steps\")\n",
    "    plt.ylabel(\"Scaled price\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for split_name, y_true, y_pred in splits:\n",
    "    if len(y_true) == 0:\n",
    "        print(f\"{split_name}: no samples available.\")\n",
    "        continue\n",
    "    mse = mean_squared_error(y_true[:, target_feature_idx], y_pred[:, target_feature_idx])\n",
    "    mae = mean_absolute_error(y_true[:, target_feature_idx], y_pred[:, target_feature_idx])\n",
    "    dir_acc = directional_accuracy(y_true, y_pred)\n",
    "    print(f\"{split_name}: MSE={mse:.6f}, MAE={mae:.6f}, directional accuracy={dir_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try alternative architectures such as deeper GRUs, LSTMs, or convolutional hybrids and compare validation metrics.\n",
    "- Incorporate additional features (e.g., volume or technical indicators) after revisiting the preprocessing pipeline.\n",
    "- Persist the fitted scaler and invert the predictions to interpret results in the original price units.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "37fed6bb",
      "metadata": {
        "id": "37fed6bb"
      },
      "source": [
        "# IMDb Sentiment Analysis with Bidirectional LSTM\n",
        "\n",
        "This notebook builds a sentiment classifier for IMDb movie reviews using TensorFlow/Keras. We walk through loading the dataset, preparing padded sequences, training a bidirectional LSTM, and evaluating performance.\n",
        "\n",
        "**Pipeline overview**\n",
        "\n",
        "1. Setup and imports  \n",
        "2. Load and inspect the IMDb dataset  \n",
        "3. Preprocess sequences with padding and exploratory checks  \n",
        "4. Build the Bidirectional LSTM Model\n",
        "5. Evaluate accuracy and error patterns, then outline next steps\n",
        "\n",
        "Dataset: TensorFlow/Keras IMDb reviews (25k train / 25k test) with integer-encoded tokens sorted by frequency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd16ef7",
      "metadata": {
        "id": "ccd16ef7"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "Configure core libraries, reproducibility seeds, and training hyperparameters used throughout the workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c64ff24",
      "metadata": {
        "id": "1c64ff24"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Check working directory\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41733915",
      "metadata": {
        "id": "41733915"
      },
      "source": [
        "## 2. Load and inspect the IMDb dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662f1d06",
      "metadata": {
        "id": "662f1d06"
      },
      "outputs": [],
      "source": [
        "# 1ï¸âƒ£ Load each split dataset from CSV\n",
        "df_o = pd.read_csv(\"imdb_adaptation.csv\")\n",
        "\n",
        "print(\"\\nðŸ”¹ Training set structure:\")\n",
        "df_o.info()          # Overview of columns, dtypes, and non-null counts\n",
        "print(\"\\nðŸ”¹ First rows of training data:\")\n",
        "print(df_o.head())   # Display first few rows to confirm format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2a0091",
      "metadata": {
        "id": "cc2a0091"
      },
      "source": [
        "## 3. Preprocess sequences with padding and exploratory checks\n",
        "This step uses padding and systematic data splitting to prepare the dataset for neural network training.\n",
        "It ensures that all sequences share the same dimensionality, that the data distribution remains consistent, and that the model can be properly validated and tested on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41445ef5",
      "metadata": {
        "id": "41445ef5"
      },
      "source": [
        "### 3.1 Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3675beba",
      "metadata": {
        "id": "3675beba"
      },
      "outputs": [],
      "source": [
        "# Convert string representations of lists back to actual lists\n",
        "from ast import literal_eval\n",
        "df_o[\"sequence\"] = df_o[\"sequence\"].apply(literal_eval)\n",
        "\n",
        "# Apply padding separately\n",
        "MAX_LEN = 200\n",
        "X_padded = pad_sequences(df_o[\"sequence\"], maxlen=MAX_LEN)\n",
        "\n",
        "# Extract labels\n",
        "y = df_o[\"label\"].values\n",
        "\n",
        "# Display some information about the processed data\n",
        "print(\"First converted sequence:\", df_o[\"sequence\"].iloc[0])\n",
        "print(\"Shape after padding:\", X_padded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9eebeed",
      "metadata": {
        "id": "d9eebeed"
      },
      "source": [
        "### 3.2 Split Train/Val/Test -> 80/10/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6b2a13",
      "metadata": {
        "id": "ac6b2a13"
      },
      "outputs": [],
      "source": [
        "# 1ï¸âƒ£ First split â†’ 80% train / 20% temp (stratified)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_padded, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# 2ï¸âƒ£ Split the 20% into 10% val / 10% test (stratified)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"âœ… Stratified data split done:\")\n",
        "for name, arr in zip([\"Train\", \"Validation\", \"Test\"], [y_train, y_val, y_test]):\n",
        "    unique, counts = np.unique(arr, return_counts=True)\n",
        "    ratio = counts / counts.sum()\n",
        "    print(f\"{name} class ratio: {dict(zip(unique, np.round(ratio, 3)))}\")\n",
        "\n",
        "#  DISPLAY FINAL SHAPES\n",
        "print(\"\\nâœ… Final dataset shapes:\")\n",
        "print(f\"Train:       {X_train.shape}\")\n",
        "print(f\"Validation:  {X_val.shape}\")\n",
        "print(f\"Test:        {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5080c87c",
      "metadata": {
        "id": "5080c87c"
      },
      "source": [
        "## 4. Build the Bidirectional LSTM Model\n",
        "\n",
        "Stack an embedding layer with a bidirectional LSTM to capture contextual information from both directions, then pool and classify with dense layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015b7ffb",
      "metadata": {
        "id": "015b7ffb"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# BUILD MODEL\n",
        "model = Sequential([\n",
        "    tf.keras.layers.Input(shape=(MAX_LEN,)),\n",
        "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(\n",
        "            units=32,\n",
        "            dropout=0.4,\n",
        "            recurrent_dropout=0.4\n",
        "        )\n",
        "    ),\n",
        "\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model with optimizer, loss, and evaluation metric\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# ============================================\n",
        "# TRAIN THE MODEL\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5, verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2cac35",
      "metadata": {
        "id": "5f2cac35"
      },
      "outputs": [],
      "source": [
        "# PLOT TRAINING CURVES\n",
        "plt.plot(history.history['loss']) # Plotting training loss\n",
        "plt.plot(history.history['val_loss']) # Plotting validation loss\n",
        "plt.title(\"Training and Validation Loss Over Epochs\") # Adding title\n",
        "plt.xlabel(\"Epochs\") # Adding x-axis label\n",
        "plt.ylabel(\"Loss\") # Adding y-axis label\n",
        "plt.legend([\"Training Loss\", \"Validation Loss\"])\n",
        "plt.show() # Displaying the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb5594f",
      "metadata": {
        "id": "edb5594f"
      },
      "source": [
        "## 5. Evaluate accuracy and error patterns\n",
        "\n",
        "After training, the modelâ€™s performance is assessed on the test set to verify its ability to generalize to unseen data.\n",
        "Besides the loss and accuracy metrics automatically tracked during training, a more detailed analysis is carried out using additional classification metrics such as Precision, Recall and F1 Score.\n",
        "\n",
        "These metrics provide a clearer understanding of how well the model distinguishes between positive and negative reviews, complementing the overall accuracy measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dcb274e",
      "metadata": {
        "id": "0dcb274e"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test set using Keras built-in evaluation\n",
        "loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy (Keras): {test_accuracy:.4f}\")\n",
        "\n",
        "# Manual evaluation using sklearn metrics\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy (Sklearn): {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da6c354",
      "metadata": {
        "id": "3da6c354"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Explore alternative model architectures (stacked LSTMs, GRUs, or convolutional front-ends)\n",
        "- Increase `NUM_WORDS` or `MAX_LEN` and observe the trade-off between vocabulary coverage and training cost.\n",
        "- Add regularisation such as dropout, or incorporate pre-trained word embeddings (e.g., GloVe) for richer representations.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "GUALdm99mwKR",
      "metadata": {
        "id": "GUALdm99mwKR"
      },
      "source": [
        "#`RNN Exercises`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3645bd0",
      "metadata": {
        "id": "e3645bd0"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Isto está formatado como código\n",
        "```\n",
        "\n",
        "# Ex1 Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f4023e",
      "metadata": {
        "id": "e1f4023e"
      },
      "source": [
        "Sentiment analysis is a computational technique used to identify and extract subjective information from text, determining the writer's attitude or opinion towards a topic. The objective of this study is to classify movie reviews from the IMDb dataset as either positive [1] or negative [0] using a Recurrent Neural Network (RNN), specifically a Bidirectional Long Short-Term Memory (LSTM) network.\n",
        "\n",
        "The dataset consists of 50,000 movie reviews, equally divided into training (25,000) and testing (25,000) sets, each labeled with a binary sentiment. The dataset has already undergone initial preprocessing, including text cleaning, lowercasing, and tokenization into integer sequences ordered by word frequency.\n",
        "\n",
        "The application of RNNs, particularly LSTMs, is highly relevant to this task as they are capable of capturing sequential dependencies and contextual information within the text, providing a deeper understanding of sentiment compared to methods based solely on word counts.\n",
        "\n",
        "The dataset is publicly available through the Keras API: https://keras.io/api/datasets/imdb/\n",
        "\n",
        "\n",
        "This exercise, is an adaptation of:\n",
        "* Deep learning for dummies. (Wiley). John Wiley & Sons. [Chapter 14]\n",
        "* https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5uUAcfomX5E5",
      "metadata": {
        "id": "5uUAcfomX5E5"
      },
      "source": [
        "## Pre-Requisites:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y2dLEZMZX90i",
      "metadata": {
        "id": "y2dLEZMZX90i"
      },
      "source": [
        "To do this exercise, we imply that you already know the basics about RNN and text Mining.\n",
        "If you are not confortable, some good references to start:\n",
        "\n",
        "* Cheat Sheet Keras: https://media.datacamp.com/legacy/image/upload/v1660903348/Keras_Cheat_Sheet_gssmi8.pdf\n",
        "* Cheat Sheet: https://github.com/BharathKumarNLP/Deep-Learning-Cheat-Sheets/blob/master/cheatsheet-recurrent-neural-networks.pdf  \n",
        "* https://github.com/DSC-SPIDAL/harpgbdt/blob/master/doc/meeting/0821-DistributedGBT/fig/Fundamentals%20of%20Predictive%20Text%20Mining%20.pdf\n",
        "*   Deep learning for dummies. (Wiley). John Wiley & Sons. [Chapter 14]: https://moodle2526.up.pt/pluginfile.php/136412/mod_folder/content/0/Deep%20Learning%20for%20Dummies.pdf?forcedownload=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cec590b",
      "metadata": {
        "id": "5cec590b"
      },
      "source": [
        "To fully understand this exercise, is fundamental to know the basic uma compreensão fundamental de RNNs e princípios de mineração de texto é benéfica. Para aqueles que buscam fortalecer seus conhecimentos nessas áreas, os seguintes recursos são recomendados:\n",
        "\n",
        "Datacamp. (n.d.). *Keras Cheat Sheet*. https://media.datacamp.com/legacy/image/upload/v1660903348/Keras_Cheat_Sheet_gssmi8.pdf\n",
        "\n",
        "Kumar, B. (n.d.). *Deep-Learning-Cheat-Sheets*. GitHub. https://github.com/BharathKumarNLP/Deep-Learning-Cheat-Sheets/blob/master/cheatsheet-recurrent-neural-networks.pdf\n",
        "\n",
        "Deep Learning for Dummies. (n.d.). https://moodle2526.up.pt/pluginfile.php/136412/mod_folder/content/0/Deep%20Learning%20for%20Dummies.pdf?forcedownload=1\n",
        "\n",
        "Nielsen, L. (n.d.). *Fundamentals of Predictive Text Mining*. GitHub. https://github.com/DSC-SPIDAL/harpgbdt/blob/master/doc/meeting/0821-DistributedGBT/fig/Fundamentals%20of%20Predictive%20Text%20Mining%20.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z_W4fPD2eQsV",
      "metadata": {
        "id": "Z_W4fPD2eQsV"
      },
      "source": [
        "## Objectives:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa8ebff",
      "metadata": {
        "id": "caa8ebff"
      },
      "source": [
        "* Classify movie reviews as positive or negative using an LSTM.\n",
        "* Apply preprocessing techniques for recurrent models.\n",
        "* Build and train a bidirectional LSTM model for sentiment analysis.\n",
        "* Evaluate the performance of the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211003ad",
      "metadata": {
        "id": "211003ad"
      },
      "source": [
        "We first pull in the core libraries—Keras for the dataset and model utilities, plus pandas for quick inspection. The IMDb dataset arrives pre-split into training and test sets, with each review encoded as a sequence of integer word indices. The data is ranked frenquency. Limiting the vocabulary to the 10,000 most frequent tokens keeps the problem manageable while preserving the most informative words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f27dd88",
      "metadata": {
        "id": "8f27dd88"
      },
      "source": [
        "## 1. Import Libraries and Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef29e594",
      "metadata": {
        "id": "ef29e594"
      },
      "source": [
        "Because reviews naturally vary in length, we use pad_sequences to reshape every sequence to a fixed size. Reviews longer than 200 tokens are truncated, and shorter reviews are left-padded with zeros. This uniform shape is essential for feeding data into the LSTM layers that expect consistent timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "80c7b127",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "80c7b127",
        "outputId": "3ee0f933-4aa3-46ef-92ca-6157ede8a149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test samples: 25000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 13, 119, 78, 3310, 102, 13, 66, 81, 13, 46...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 4, 86, 173, 3007, 11, 4195, 9, 44, 15, 416...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 449, 2, 50, 26, 38, 111, 85, 108, 13, 181,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 25, 3525, 119, 4, 954, 364, 352, 102, 14, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 13, 81, 79, 7937, 19, 682, 5111, 7, 2282, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  [1, 13, 119, 78, 3310, 102, 13, 66, 81, 13, 46...\n",
              "1  [1, 4, 86, 173, 3007, 11, 4195, 9, 44, 15, 416...\n",
              "2  [1, 449, 2, 50, 26, 38, 111, 85, 108, 13, 181,...\n",
              "3  [1, 25, 3525, 119, 4, 954, 364, 352, 102, 14, ...\n",
              "4  [1, 13, 81, 79, 7937, 19, 682, 5111, 7, 2282, ..."
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- Import core libraries ---\n",
        "\n",
        "from keras.datasets import imdb       # IMDb dataset from Keras\n",
        "import pandas as pd                   # To handle tabular data for exploration\n",
        "\n",
        "# --- Load the IMDb dataset ---\n",
        "\n",
        "# Keep only the 10,000 most frequent words (to limit vocabulary size)\n",
        "top_words = 10000\n",
        "\n",
        "# The dataset is already split into training and testing sets.\n",
        "# Each review is represented as a sequence of integer word indices.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words, seed=21)\n",
        "\n",
        "# --- Convert to DataFrames for a quick look ---\n",
        "\n",
        "df_x_train = pd.DataFrame(x_train)\n",
        "df_y_train = pd.DataFrame(y_train)\n",
        "df_x_test = pd.DataFrame(x_test)\n",
        "df_y_test = pd.DataFrame(y_test)\n",
        "\n",
        "# Print the number of samples in the test set\n",
        "print(f\"Number of test samples: {len(x_test)}\")\n",
        "\n",
        "# Show first few reviews\n",
        "df_x_train.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2176ef",
      "metadata": {
        "id": "4b2176ef"
      },
      "source": [
        "## 2. Preprocessing — Sequence Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "104c6ae3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "104c6ae3",
        "outputId": "f1db005f-771c-4f95-f3b9-9ccf8e7f0eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example of a padded sequence:\n",
            "[  61  492   16 3953  159   29 1131   13 2134 3872   81   41   32   14\n",
            "  832   56    8   35  576 1301    5 5348 3134  255  335  170    8    2\n",
            "   72 1168 1656   57   29    9    2    2 3310  415   11 5215   89 1047\n",
            "   10   10   81   24  106   14   20  126]\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum review length\n",
        "max_pad = 50\n",
        "\n",
        "# Pad or truncate all sequences to the same length (200 words)\n",
        "x_train = pad_sequences(x_train, maxlen=max_pad)\n",
        "x_test = pad_sequences(x_test, maxlen=max_pad)\n",
        "\n",
        "# Check one padded review\n",
        "print(\"Example of a padded sequence:\")\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b57e00",
      "metadata": {
        "id": "79b57e00"
      },
      "source": [
        "## 3. Model Architecture — Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d920af",
      "metadata": {
        "id": "e0d920af"
      },
      "source": [
        "The model couples an embedding layer with a bidirectional LSTM, allowing it to read each review from both directions and capture context that might otherwise be lost. Stacking dense layers on top prepares a rich representation that culminates in a single sigmoid neuron for binary sentiment predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "941a2027",
      "metadata": {
        "id": "941a2027"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPool1D\n",
        "\n",
        "# --- Define hyperparameters ---\n",
        "embedding_vector_length = 32  # size of the word embeddings\n",
        "\n",
        "# --- Build the model ---\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Embedding layer: converts each word ID into a dense 32-dimensional vector\n",
        "model.add(Embedding(input_dim=top_words, output_dim=embedding_vector_length))\n",
        "\n",
        "# 2. Bidirectional LSTM: processes the text both forward and backward\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "\n",
        "# 3. Global max pooling: reduces the sequence output into a single feature vector\n",
        "model.add(GlobalMaxPool1D())\n",
        "\n",
        "# 4. Dense layer with ReLU: learns non-linear combinations of the extracted features\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# 5. Dropout: randomly disables neurons (50%) to prevent overfitting\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 6. Output layer: single neuron with sigmoid for binary sentiment classification\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd1da48",
      "metadata": {
        "id": "7cd1da48"
      },
      "source": [
        "## 4. Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ad3ffbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "9ad3ffbe",
        "outputId": "86da7051-3f50-4baa-a580-93c12dd7912b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">320,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │       \u001b[38;5;34m320,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m49,664\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">373,825</span> (1.43 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m373,825\u001b[0m (1.43 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">373,825</span> (1.43 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m373,825\u001b[0m (1.43 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build the model (optional but ensures summary displays input/output shapes)\n",
        "model.build((None, 200))\n",
        "\n",
        "# Summary of the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "724abd1d",
      "metadata": {
        "id": "724abd1d"
      },
      "source": [
        "## 5. Model Compilation and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7c0696",
      "metadata": {
        "id": "5c7c0696"
      },
      "source": [
        "We compile the network with binary_crossentropy, the standard loss for positive/negative classification, and the adaptive Adam optimizer. Training runs for a few epochs while tracking validation accuracy, which helps us spot overfitting or underfitting quickly. The final evaluation on the held-out test set summarizes how well the model generalises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "402ed0de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "402ed0de",
        "outputId": "84abf6b7-4931-4a05-9470-069907555be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.7332 - loss: 0.5159 - val_accuracy: 0.8162 - val_loss: 0.4061\n",
            "Epoch 2/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.8516 - loss: 0.3571 - val_accuracy: 0.8138 - val_loss: 0.4044\n",
            "Epoch 3/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8929 - loss: 0.2812"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# Binary sentiment (0 or 1)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,             \u001b[38;5;66;03m# Adaptive gradient optimizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]          \u001b[38;5;66;03m# Evaluate model performance\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Number of full passes through the data\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Number of samples per gradient update\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Evaluate on test data each epoch\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:401\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    392\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    393\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m     )\n\u001b[1;32m--> 401\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    413\u001b[0m }\n\u001b[0;32m    414\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:489\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    488\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(begin_step)\n\u001b[1;32m--> 489\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(end_step, logs)\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',   # Binary sentiment (0 or 1)\n",
        "    optimizer='adam',             # Adaptive gradient optimizer\n",
        "    metrics=['accuracy']          # Evaluate model performance\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,                     # Number of full passes through the data\n",
        "    batch_size=64,               # Number of samples per gradient update\n",
        "    validation_data=(x_test, y_test),  # Evaluate on test data each epoch\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c80a2160",
      "metadata": {
        "id": "c80a2160"
      },
      "outputs": [],
      "source": [
        "# Evaluate model performance on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4fe1a5",
      "metadata": {
        "id": "ec4fe1a5"
      },
      "source": [
        "## Final Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "549104c6",
      "metadata": {
        "id": "549104c6"
      },
      "source": [
        "Accuracy in the mid-80% range is typical for this setup, and there is room to experiment—try deeper embeddings, longer padded sequences, or alternative recurrent layers such as GRUs or Conv1D blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AxiFHi7RtX_j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxiFHi7RtX_j",
        "outputId": "1d878951-3553-45bd-fe35-dfa7355e2d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class Bidirectional in module keras.src.layers.rnn.bidirectional:\n",
            "\n",
            "class Bidirectional(keras.src.layers.layer.Layer)\n",
            " |  Bidirectional(layer, merge_mode='concat', weights=None, backward_layer=None, **kwargs)\n",
            " |\n",
            " |  Bidirectional wrapper for RNNs.\n",
            " |\n",
            " |  Args:\n",
            " |      layer: `keras.layers.RNN` instance, such as\n",
            " |          `keras.layers.LSTM` or `keras.layers.GRU`.\n",
            " |          It could also be a `keras.layers.Layer` instance\n",
            " |          that meets the following criteria:\n",
            " |          1. Be a sequence-processing layer (accepts 3D+ inputs).\n",
            " |          2. Have a `go_backwards`, `return_sequences` and `return_state`\n",
            " |          attribute (with the same semantics as for the `RNN` class).\n",
            " |          3. Have an `input_spec` attribute.\n",
            " |          4. Implement serialization via `get_config()` and `from_config()`.\n",
            " |          Note that the recommended way to create new RNN layers is to write a\n",
            " |          custom RNN cell and use it with `keras.layers.RNN`, instead of\n",
            " |          subclassing `keras.layers.Layer` directly.\n",
            " |          When `return_sequences` is `True`, the output of the masked\n",
            " |          timestep will be zero regardless of the layer's original\n",
            " |          `zero_output_for_mask` value.\n",
            " |      merge_mode: Mode by which outputs of the forward and backward RNNs\n",
            " |          will be combined. One of `{\"sum\", \"mul\", \"concat\", \"ave\", None}`.\n",
            " |          If `None`, the outputs will not be combined,\n",
            " |          they will be returned as a list. Defaults to `\"concat\"`.\n",
            " |      backward_layer: Optional `keras.layers.RNN`,\n",
            " |          or `keras.layers.Layer` instance to be used to handle\n",
            " |          backwards input processing.\n",
            " |          If `backward_layer` is not provided, the layer instance passed\n",
            " |          as the `layer` argument will be used to generate the backward layer\n",
            " |          automatically.\n",
            " |          Note that the provided `backward_layer` layer should have properties\n",
            " |          matching those of the `layer` argument, in particular\n",
            " |          it should have the same values for `stateful`, `return_states`,\n",
            " |          `return_sequences`, etc. In addition, `backward_layer`\n",
            " |          and `layer` should have different `go_backwards` argument values.\n",
            " |          A `ValueError` will be raised if these requirements are not met.\n",
            " |\n",
            " |  Call arguments:\n",
            " |      The call arguments for this layer are the same as those of the\n",
            " |      wrapped RNN layer. Beware that when passing the `initial_state`\n",
            " |      argument during the call of this layer, the first half in the\n",
            " |      list of elements in the `initial_state` list will be passed to\n",
            " |      the forward RNN call and the last half in the list of elements\n",
            " |      will be passed to the backward RNN call.\n",
            " |\n",
            " |  Note: instantiating a `Bidirectional` layer from an existing RNN layer\n",
            " |  instance will not reuse the weights state of the RNN layer instance -- the\n",
            " |  `Bidirectional` layer will have freshly initialized weights.\n",
            " |\n",
            " |  Examples:\n",
            " |\n",
            " |  ```python\n",
            " |  model = Sequential([\n",
            " |      Input(shape=(5, 10)),\n",
            " |      Bidirectional(LSTM(10, return_sequences=True),\n",
            " |      Bidirectional(LSTM(10)),\n",
            " |      Dense(5, activation=\"softmax\"),\n",
            " |  ])\n",
            " |  model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
            " |\n",
            " |  # With custom backward layer\n",
            " |  forward_layer = LSTM(10, return_sequences=True)\n",
            " |  backward_layer = LSTM(10, activation='relu', return_sequences=True,\n",
            " |                        go_backwards=True)\n",
            " |  model = Sequential([\n",
            " |      Input(shape=(5, 10)),\n",
            " |      Bidirectional(forward_layer, backward_layer=backward_layer),\n",
            " |      Dense(5, activation=\"softmax\"),\n",
            " |  ])\n",
            " |  model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
            " |  ```\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      Bidirectional\n",
            " |      keras.src.layers.layer.Layer\n",
            " |      keras.src.backend.tensorflow.layer.TFLayer\n",
            " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
            " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
            " |      tensorflow.python.trackable.base.Trackable\n",
            " |      keras.src.ops.operation.Operation\n",
            " |      keras.src.saving.keras_saveable.KerasSaveable\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, layer, merge_mode='concat', weights=None, backward_layer=None, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  build(self, sequences_shape, initial_state_shape=None)\n",
            " |\n",
            " |  call(self, sequences, initial_state=None, mask=None, training=None)\n",
            " |\n",
            " |  compute_mask(self, _, mask)\n",
            " |\n",
            " |  compute_output_shape(self, sequences_shape, initial_state_shape=None)\n",
            " |\n",
            " |  get_config(self)\n",
            " |      Returns the config of the object.\n",
            " |\n",
            " |      An object config is a Python dictionary (serializable)\n",
            " |      containing the information needed to re-instantiate it.\n",
            " |\n",
            " |  reset_state(self)\n",
            " |\n",
            " |  reset_states(self)\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |\n",
            " |  from_config(config, custom_objects=None)\n",
            " |      Creates an operation from its config.\n",
            " |\n",
            " |      This method is the reverse of `get_config`, capable of instantiating the\n",
            " |      same operation from the config dictionary.\n",
            " |\n",
            " |      Note: If you override this method, you might receive a serialized dtype\n",
            " |      config, which is a `dict`. You can deserialize it as follows:\n",
            " |\n",
            " |      ```python\n",
            " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
            " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |          config: A Python dictionary, typically the output of `get_config`.\n",
            " |\n",
            " |      Returns:\n",
            " |          An operation instance.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |\n",
            " |  states\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __annotations__ = {}\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.layers.layer.Layer:\n",
            " |\n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Call self as a function.\n",
            " |\n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |\n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |\n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |\n",
            " |  add_loss(self, loss)\n",
            " |      Can be called inside of the `call()` method to add a scalar loss.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```python\n",
            " |      class MyLayer(Layer):\n",
            " |          ...\n",
            " |          def call(self, x):\n",
            " |              self.add_loss(ops.sum(x))\n",
            " |              return x\n",
            " |      ```\n",
            " |\n",
            " |  add_metric(self, *args, **kwargs)\n",
            " |\n",
            " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
            " |      Add a weight variable to the layer.\n",
            " |\n",
            " |      Alias of `add_weight()`.\n",
            " |\n",
            " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='none', overwrite_with_gradient=False, name=None)\n",
            " |      Add a weight variable to the layer.\n",
            " |\n",
            " |      Args:\n",
            " |          shape: Shape tuple for the variable. Must be fully-defined\n",
            " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
            " |          initializer: Initializer object to use to populate the initial\n",
            " |              variable value, or string name of a built-in initializer\n",
            " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
            " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
            " |              for all other types (e.g. int, bool).\n",
            " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
            " |              unspecified, defaults to the layer's variable dtype\n",
            " |              (which itself defaults to `\"float32\"` if unspecified).\n",
            " |          trainable: Boolean, whether the variable should be trainable via\n",
            " |              backprop or whether its updates are managed manually. Defaults\n",
            " |              to `True`.\n",
            " |          autocast: Boolean, whether to autocast layers variables when\n",
            " |              accessing them. Defaults to `True`.\n",
            " |          regularizer: Regularizer object to call to apply penalty on the\n",
            " |              weight. These penalties are summed into the loss function\n",
            " |              during optimization. Defaults to `None`.\n",
            " |          constraint: Contrainst object to call on the variable after any\n",
            " |              optimizer update, or string name of a built-in constraint.\n",
            " |              Defaults to `None`.\n",
            " |          aggregation: Optional string, one of `None`, `\"none\"`, `\"mean\"`,\n",
            " |              `\"sum\"` or `\"only_first_replica\"`. Annotates the variable with\n",
            " |              the type of multi-replica aggregation to be used for this\n",
            " |              variable when writing custom data parallel training loops.\n",
            " |              Defaults to `\"none\"`.\n",
            " |          overwrite_with_gradient: Boolean, whether to overwrite the variable\n",
            " |              with the computed gradient. This is useful for float8 training.\n",
            " |              Defaults to `False`.\n",
            " |          name: String name of the variable. Useful for debugging purposes.\n",
            " |\n",
            " |  build_from_config(self, config)\n",
            " |      Builds the layer's states with the supplied config dict.\n",
            " |\n",
            " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
            " |      which creates weights based on the layer's input shape in the supplied\n",
            " |      config. If your config contains other information needed to load the\n",
            " |      layer's state, you should override this method.\n",
            " |\n",
            " |      Args:\n",
            " |          config: Dict containing the input shape associated with this layer.\n",
            " |\n",
            " |  compute_output_spec(self, *args, **kwargs)\n",
            " |\n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |\n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |\n",
            " |  get_build_config(self)\n",
            " |      Returns a dictionary with the layer's input shape.\n",
            " |\n",
            " |      This method returns a config dict that can be used by\n",
            " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
            " |      Lookup tables) needed by the layer.\n",
            " |\n",
            " |      By default, the config only contains the input shape that the layer\n",
            " |      was built with. If you're writing a custom layer that creates state in\n",
            " |      an unusual way, you should override this method to make sure this state\n",
            " |      is already created when Keras attempts to load its value upon model\n",
            " |      loading.\n",
            " |\n",
            " |      Returns:\n",
            " |          A dict containing the input shape associated with the layer.\n",
            " |\n",
            " |  get_weights(self)\n",
            " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
            " |\n",
            " |  load_own_variables(self, store)\n",
            " |      Loads the state of the layer.\n",
            " |\n",
            " |      You can override this method to take full control of how the state of\n",
            " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
            " |\n",
            " |      Args:\n",
            " |          store: Dict from which the state of the model will be loaded.\n",
            " |\n",
            " |  quantize(self, mode, type_check=True)\n",
            " |\n",
            " |  quantized_build(self, input_shape, mode)\n",
            " |\n",
            " |  quantized_call(self, *args, **kwargs)\n",
            " |\n",
            " |  rematerialized_call(self, layer_call, *args, **kwargs)\n",
            " |      Enable rematerialization dynamically for layer's call method.\n",
            " |\n",
            " |      Args:\n",
            " |          layer_call: The original `call` method of a layer.\n",
            " |\n",
            " |      Returns:\n",
            " |          Rematerialized layer's `call` method.\n",
            " |\n",
            " |  save_own_variables(self, store)\n",
            " |      Saves the state of the layer.\n",
            " |\n",
            " |      You can override this method to take full control of how the state of\n",
            " |      the layer is saved upon calling `model.save()`.\n",
            " |\n",
            " |      Args:\n",
            " |          store: Dict where the state of the model will be saved.\n",
            " |\n",
            " |  set_weights(self, weights)\n",
            " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
            " |\n",
            " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
            " |      Call the layer without any side effects.\n",
            " |\n",
            " |      Args:\n",
            " |          trainable_variables: List of trainable variables of the model.\n",
            " |          non_trainable_variables: List of non-trainable variables of the\n",
            " |              model.\n",
            " |          *args: Positional arguments to be passed to `call()`.\n",
            " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
            " |              losses created during `call()` as part of its return values.\n",
            " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
            " |\n",
            " |      Returns:\n",
            " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
            " |              If `return_losses = True`, then returns\n",
            " |              `(outputs, non_trainable_variables, losses)`.\n",
            " |\n",
            " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
            " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
            " |      (if there are any random operations part of the layer, such as dropout),\n",
            " |      and `Metric` state (if there are any metrics attached to the layer).\n",
            " |      These are all elements of state of the layer.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```python\n",
            " |      model = ...\n",
            " |      data = ...\n",
            " |      trainable_variables = model.trainable_variables\n",
            " |      non_trainable_variables = model.non_trainable_variables\n",
            " |      # Call the model with zero side effects\n",
            " |      outputs, non_trainable_variables = model.stateless_call(\n",
            " |          trainable_variables,\n",
            " |          non_trainable_variables,\n",
            " |          data,\n",
            " |      )\n",
            " |      # Attach the updated state to the model\n",
            " |      # (until you do this, the model is still in its pre-call state).\n",
            " |      for ref_var, value in zip(\n",
            " |          model.non_trainable_variables, non_trainable_variables\n",
            " |      ):\n",
            " |          ref_var.assign(value)\n",
            " |      ```\n",
            " |\n",
            " |  symbolic_call(self, *args, **kwargs)\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from keras.src.layers.layer.Layer:\n",
            " |\n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
            " |\n",
            " |  compute_dtype\n",
            " |      The dtype of the computations performed by the layer.\n",
            " |\n",
            " |  dtype\n",
            " |      Alias of `layer.variable_dtype`.\n",
            " |\n",
            " |  input_dtype\n",
            " |      The dtype layer inputs should be converted to.\n",
            " |\n",
            " |  losses\n",
            " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
            " |\n",
            " |  metrics\n",
            " |      List of all metrics.\n",
            " |\n",
            " |  metrics_variables\n",
            " |      List of all metric variables.\n",
            " |\n",
            " |  non_trainable_variables\n",
            " |      List of all non-trainable layer state.\n",
            " |\n",
            " |      This extends `layer.non_trainable_weights` to include all state used by\n",
            " |      the layer including state for metrics and `SeedGenerator`s.\n",
            " |\n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weight variables of the layer.\n",
            " |\n",
            " |      These are the weights that should not be updated by the optimizer during\n",
            " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
            " |      state and random seeds.\n",
            " |\n",
            " |  path\n",
            " |      The path of the layer.\n",
            " |\n",
            " |      If the layer has not been built yet, it will be `None`.\n",
            " |\n",
            " |  quantization_mode\n",
            " |      The quantization mode of this layer, `None` if not quantized.\n",
            " |\n",
            " |  trainable_variables\n",
            " |      List of all trainable layer state.\n",
            " |\n",
            " |      This is equivalent to `layer.trainable_weights`.\n",
            " |\n",
            " |  trainable_weights\n",
            " |      List of all trainable weight variables of the layer.\n",
            " |\n",
            " |      These are the weights that get updated by the optimizer during training.\n",
            " |\n",
            " |  variable_dtype\n",
            " |      The dtype of the state (weights) of the layer.\n",
            " |\n",
            " |  variables\n",
            " |      List of all layer state, including random seeds.\n",
            " |\n",
            " |      This extends `layer.weights` to include all state used by the layer\n",
            " |      including `SeedGenerator`s.\n",
            " |\n",
            " |      Note that metrics variables are not included here, use\n",
            " |      `metrics_variables` to visit all the metric variables.\n",
            " |\n",
            " |  weights\n",
            " |      List of all weight variables of the layer.\n",
            " |\n",
            " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
            " |\n",
            " |  dtype_policy\n",
            " |\n",
            " |  input_spec\n",
            " |\n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |\n",
            " |  trainable\n",
            " |      Settable boolean, whether this layer should be trainable or not.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
            " |\n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a symbolic operation.\n",
            " |\n",
            " |      Only returns the tensor(s) corresponding to the *first time*\n",
            " |      the operation was called.\n",
            " |\n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |\n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |\n",
            " |      Only returns the tensor(s) corresponding to the *first time*\n",
            " |      the operation was called.\n",
            " |\n",
            " |      Returns:\n",
            " |          Output tensor or list of output tensors.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
            " |\n",
            " |  __reduce__(self)\n",
            " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
            " |\n",
            " |      The method returns a tuple of two elements: a function, and a list of\n",
            " |      arguments to pass to that function.  In this case we just leverage the\n",
            " |      keras saving library.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(Bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "FPvrfvWgtcyE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPvrfvWgtcyE",
        "outputId": "0780fa33-729f-4142-b96d-6c1ce5b471ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class LSTM in module keras.src.layers.rnn.lstm:\n",
            "\n",
            "class LSTM(keras.src.layers.rnn.rnn.RNN)\n",
            " |  LSTM(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, seed=None, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, use_cudnn='auto', **kwargs)\n",
            " |  \n",
            " |  Long Short-Term Memory layer - Hochreiter 1997.\n",
            " |  \n",
            " |  Based on available runtime hardware and constraints, this layer\n",
            " |  will choose different implementations (cuDNN-based or backend-native)\n",
            " |  to maximize the performance. If a GPU is available and all\n",
            " |  the arguments to the layer meet the requirement of the cuDNN kernel\n",
            " |  (see below for details), the layer will use a fast cuDNN implementation\n",
            " |  when using the TensorFlow backend.\n",
            " |  The requirements to use the cuDNN implementation are:\n",
            " |  \n",
            " |  1. `activation` == `tanh`\n",
            " |  2. `recurrent_activation` == `sigmoid`\n",
            " |  3. `recurrent_dropout` == 0\n",
            " |  4. `unroll` is `False`\n",
            " |  5. `use_bias` is `True`\n",
            " |  6. Inputs, if use masking, are strictly right-padded.\n",
            " |  7. Eager execution is enabled in the outermost context.\n",
            " |  \n",
            " |  For example:\n",
            " |  \n",
            " |  >>> inputs = np.random.random((32, 10, 8))\n",
            " |  >>> lstm = keras.layers.LSTM(4)\n",
            " |  >>> output = lstm(inputs)\n",
            " |  >>> output.shape\n",
            " |  (32, 4)\n",
            " |  >>> lstm = keras.layers.LSTM(\n",
            " |  ...     4, return_sequences=True, return_state=True)\n",
            " |  >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
            " |  >>> whole_seq_output.shape\n",
            " |  (32, 10, 4)\n",
            " |  >>> final_memory_state.shape\n",
            " |  (32, 4)\n",
            " |  >>> final_carry_state.shape\n",
            " |  (32, 4)\n",
            " |  \n",
            " |  Args:\n",
            " |      units: Positive integer, dimensionality of the output space.\n",
            " |      activation: Activation function to use.\n",
            " |          Default: hyperbolic tangent (`tanh`).\n",
            " |          If you pass `None`, no activation is applied\n",
            " |          (ie. \"linear\" activation: `a(x) = x`).\n",
            " |      recurrent_activation: Activation function to use\n",
            " |          for the recurrent step.\n",
            " |          Default: sigmoid (`sigmoid`).\n",
            " |          If you pass `None`, no activation is applied\n",
            " |          (ie. \"linear\" activation: `a(x) = x`).\n",
            " |      use_bias: Boolean, (default `True`), whether the layer\n",
            " |          should use a bias vector.\n",
            " |      kernel_initializer: Initializer for the `kernel` weights matrix,\n",
            " |          used for the linear transformation of the inputs. Default:\n",
            " |          `\"glorot_uniform\"`.\n",
            " |      recurrent_initializer: Initializer for the `recurrent_kernel`\n",
            " |          weights matrix, used for the linear transformation of the recurrent\n",
            " |          state. Default: `\"orthogonal\"`.\n",
            " |      bias_initializer: Initializer for the bias vector. Default: `\"zeros\"`.\n",
            " |      unit_forget_bias: Boolean (default `True`). If `True`,\n",
            " |          add 1 to the bias of the forget gate at initialization.\n",
            " |          Setting it to `True` will also force `bias_initializer=\"zeros\"`.\n",
            " |          This is recommended in [Jozefowicz et al.](\n",
            " |          https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)\n",
            " |      kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
            " |          matrix. Default: `None`.\n",
            " |      recurrent_regularizer: Regularizer function applied to the\n",
            " |          `recurrent_kernel` weights matrix. Default: `None`.\n",
            " |      bias_regularizer: Regularizer function applied to the bias vector.\n",
            " |          Default: `None`.\n",
            " |      activity_regularizer: Regularizer function applied to the output of the\n",
            " |          layer (its \"activation\"). Default: `None`.\n",
            " |      kernel_constraint: Constraint function applied to the `kernel` weights\n",
            " |          matrix. Default: `None`.\n",
            " |      recurrent_constraint: Constraint function applied to the\n",
            " |          `recurrent_kernel` weights matrix. Default: `None`.\n",
            " |      bias_constraint: Constraint function applied to the bias vector.\n",
            " |          Default: `None`.\n",
            " |      dropout: Float between 0 and 1. Fraction of the units to drop for the\n",
            " |          linear transformation of the inputs. Default: 0.\n",
            " |      recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n",
            " |          for the linear transformation of the recurrent state. Default: 0.\n",
            " |      seed: Random seed for dropout.\n",
            " |      return_sequences: Boolean. Whether to return the last output\n",
            " |          in the output sequence, or the full sequence. Default: `False`.\n",
            " |      return_state: Boolean. Whether to return the last state in addition\n",
            " |          to the output. Default: `False`.\n",
            " |      go_backwards: Boolean (default: `False`).\n",
            " |          If `True`, process the input sequence backwards and return the\n",
            " |          reversed sequence.\n",
            " |      stateful: Boolean (default: `False`). If `True`, the last state\n",
            " |          for each sample at index i in a batch will be used as initial\n",
            " |          state for the sample of index i in the following batch.\n",
            " |      unroll: Boolean (default False).\n",
            " |          If `True`, the network will be unrolled,\n",
            " |          else a symbolic loop will be used.\n",
            " |          Unrolling can speed-up a RNN,\n",
            " |          although it tends to be more memory-intensive.\n",
            " |          Unrolling is only suitable for short sequences.\n",
            " |      use_cudnn: Whether to use a cuDNN-backed implementation. `\"auto\"` will\n",
            " |          attempt to use cuDNN when feasible, and will fallback to the\n",
            " |          default implementation if not.\n",
            " |  \n",
            " |  Call arguments:\n",
            " |      inputs: A 3D tensor, with shape `(batch, timesteps, feature)`.\n",
            " |      mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n",
            " |          a given timestep should be masked  (optional).\n",
            " |          An individual `True` entry indicates that the corresponding timestep\n",
            " |          should be utilized, while a `False` entry indicates that the\n",
            " |          corresponding timestep should be ignored. Defaults to `None`.\n",
            " |      training: Python boolean indicating whether the layer should behave in\n",
            " |          training mode or in inference mode. This argument is passed to the\n",
            " |          cell when calling it. This is only relevant if `dropout` or\n",
            " |          `recurrent_dropout` is used  (optional). Defaults to `None`.\n",
            " |      initial_state: List of initial state tensors to be passed to the first\n",
            " |          call of the cell (optional, `None` causes creation\n",
            " |          of zero-filled initial state tensors). Defaults to `None`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LSTM\n",
            " |      keras.src.layers.rnn.rnn.RNN\n",
            " |      keras.src.layers.layer.Layer\n",
            " |      keras.src.backend.tensorflow.layer.TFLayer\n",
            " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
            " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
            " |      tensorflow.python.trackable.base.Trackable\n",
            " |      keras.src.ops.operation.Operation\n",
            " |      keras.src.saving.keras_saveable.KerasSaveable\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, seed=None, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, use_cudnn='auto', **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  call(self, sequences, initial_state=None, mask=None, training=False)\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the object.\n",
            " |      \n",
            " |      An object config is a Python dictionary (serializable)\n",
            " |      containing the information needed to re-instantiate it.\n",
            " |  \n",
            " |  inner_loop(self, sequences, initial_state, mask, training=False)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config)\n",
            " |      Creates an operation from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`, capable of instantiating the\n",
            " |      same operation from the config dictionary.\n",
            " |      \n",
            " |      Note: If you override this method, you might receive a serialized dtype\n",
            " |      config, which is a `dict`. You can deserialize it as follows:\n",
            " |      \n",
            " |      ```python\n",
            " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
            " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A Python dictionary, typically the output of `get_config`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An operation instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  activation\n",
            " |  \n",
            " |  bias_constraint\n",
            " |  \n",
            " |  bias_initializer\n",
            " |  \n",
            " |  bias_regularizer\n",
            " |  \n",
            " |  dropout\n",
            " |  \n",
            " |  kernel_constraint\n",
            " |  \n",
            " |  kernel_initializer\n",
            " |  \n",
            " |  kernel_regularizer\n",
            " |  \n",
            " |  recurrent_activation\n",
            " |  \n",
            " |  recurrent_constraint\n",
            " |  \n",
            " |  recurrent_dropout\n",
            " |  \n",
            " |  recurrent_initializer\n",
            " |  \n",
            " |  recurrent_regularizer\n",
            " |  \n",
            " |  unit_forget_bias\n",
            " |  \n",
            " |  units\n",
            " |  \n",
            " |  use_bias\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.layers.rnn.rnn.RNN:\n",
            " |  \n",
            " |  build(self, sequences_shape, initial_state_shape=None)\n",
            " |  \n",
            " |  compute_mask(self, _, mask)\n",
            " |  \n",
            " |  compute_output_shape(self, sequences_shape, initial_state_shape=None)\n",
            " |  \n",
            " |  get_initial_state(self, batch_size)\n",
            " |  \n",
            " |  reset_state(self)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.layers.layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Call self as a function.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  add_loss(self, loss)\n",
            " |      Can be called inside of the `call()` method to add a scalar loss.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(Layer):\n",
            " |          ...\n",
            " |          def call(self, x):\n",
            " |              self.add_loss(ops.sum(x))\n",
            " |              return x\n",
            " |      ```\n",
            " |  \n",
            " |  add_metric(self, *args, **kwargs)\n",
            " |  \n",
            " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
            " |      Add a weight variable to the layer.\n",
            " |      \n",
            " |      Alias of `add_weight()`.\n",
            " |  \n",
            " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='none', overwrite_with_gradient=False, name=None)\n",
            " |      Add a weight variable to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |          shape: Shape tuple for the variable. Must be fully-defined\n",
            " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
            " |          initializer: Initializer object to use to populate the initial\n",
            " |              variable value, or string name of a built-in initializer\n",
            " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
            " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
            " |              for all other types (e.g. int, bool).\n",
            " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
            " |              unspecified, defaults to the layer's variable dtype\n",
            " |              (which itself defaults to `\"float32\"` if unspecified).\n",
            " |          trainable: Boolean, whether the variable should be trainable via\n",
            " |              backprop or whether its updates are managed manually. Defaults\n",
            " |              to `True`.\n",
            " |          autocast: Boolean, whether to autocast layers variables when\n",
            " |              accessing them. Defaults to `True`.\n",
            " |          regularizer: Regularizer object to call to apply penalty on the\n",
            " |              weight. These penalties are summed into the loss function\n",
            " |              during optimization. Defaults to `None`.\n",
            " |          constraint: Contrainst object to call on the variable after any\n",
            " |              optimizer update, or string name of a built-in constraint.\n",
            " |              Defaults to `None`.\n",
            " |          aggregation: Optional string, one of `None`, `\"none\"`, `\"mean\"`,\n",
            " |              `\"sum\"` or `\"only_first_replica\"`. Annotates the variable with\n",
            " |              the type of multi-replica aggregation to be used for this\n",
            " |              variable when writing custom data parallel training loops.\n",
            " |              Defaults to `\"none\"`.\n",
            " |          overwrite_with_gradient: Boolean, whether to overwrite the variable\n",
            " |              with the computed gradient. This is useful for float8 training.\n",
            " |              Defaults to `False`.\n",
            " |          name: String name of the variable. Useful for debugging purposes.\n",
            " |  \n",
            " |  build_from_config(self, config)\n",
            " |      Builds the layer's states with the supplied config dict.\n",
            " |      \n",
            " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
            " |      which creates weights based on the layer's input shape in the supplied\n",
            " |      config. If your config contains other information needed to load the\n",
            " |      layer's state, you should override this method.\n",
            " |      \n",
            " |      Args:\n",
            " |          config: Dict containing the input shape associated with this layer.\n",
            " |  \n",
            " |  compute_output_spec(self, *args, **kwargs)\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |  \n",
            " |  get_build_config(self)\n",
            " |      Returns a dictionary with the layer's input shape.\n",
            " |      \n",
            " |      This method returns a config dict that can be used by\n",
            " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
            " |      Lookup tables) needed by the layer.\n",
            " |      \n",
            " |      By default, the config only contains the input shape that the layer\n",
            " |      was built with. If you're writing a custom layer that creates state in\n",
            " |      an unusual way, you should override this method to make sure this state\n",
            " |      is already created when Keras attempts to load its value upon model\n",
            " |      loading.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A dict containing the input shape associated with the layer.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
            " |  \n",
            " |  load_own_variables(self, store)\n",
            " |      Loads the state of the layer.\n",
            " |      \n",
            " |      You can override this method to take full control of how the state of\n",
            " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
            " |      \n",
            " |      Args:\n",
            " |          store: Dict from which the state of the model will be loaded.\n",
            " |  \n",
            " |  quantize(self, mode, type_check=True)\n",
            " |  \n",
            " |  quantized_build(self, input_shape, mode)\n",
            " |  \n",
            " |  quantized_call(self, *args, **kwargs)\n",
            " |  \n",
            " |  rematerialized_call(self, layer_call, *args, **kwargs)\n",
            " |      Enable rematerialization dynamically for layer's call method.\n",
            " |      \n",
            " |      Args:\n",
            " |          layer_call: The original `call` method of a layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Rematerialized layer's `call` method.\n",
            " |  \n",
            " |  save_own_variables(self, store)\n",
            " |      Saves the state of the layer.\n",
            " |      \n",
            " |      You can override this method to take full control of how the state of\n",
            " |      the layer is saved upon calling `model.save()`.\n",
            " |      \n",
            " |      Args:\n",
            " |          store: Dict where the state of the model will be saved.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
            " |  \n",
            " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
            " |      Call the layer without any side effects.\n",
            " |      \n",
            " |      Args:\n",
            " |          trainable_variables: List of trainable variables of the model.\n",
            " |          non_trainable_variables: List of non-trainable variables of the\n",
            " |              model.\n",
            " |          *args: Positional arguments to be passed to `call()`.\n",
            " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
            " |              losses created during `call()` as part of its return values.\n",
            " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
            " |              If `return_losses = True`, then returns\n",
            " |              `(outputs, non_trainable_variables, losses)`.\n",
            " |      \n",
            " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
            " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
            " |      (if there are any random operations part of the layer, such as dropout),\n",
            " |      and `Metric` state (if there are any metrics attached to the layer).\n",
            " |      These are all elements of state of the layer.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      model = ...\n",
            " |      data = ...\n",
            " |      trainable_variables = model.trainable_variables\n",
            " |      non_trainable_variables = model.non_trainable_variables\n",
            " |      # Call the model with zero side effects\n",
            " |      outputs, non_trainable_variables = model.stateless_call(\n",
            " |          trainable_variables,\n",
            " |          non_trainable_variables,\n",
            " |          data,\n",
            " |      )\n",
            " |      # Attach the updated state to the model\n",
            " |      # (until you do this, the model is still in its pre-call state).\n",
            " |      for ref_var, value in zip(\n",
            " |          model.non_trainable_variables, non_trainable_variables\n",
            " |      ):\n",
            " |          ref_var.assign(value)\n",
            " |      ```\n",
            " |  \n",
            " |  symbolic_call(self, *args, **kwargs)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from keras.src.layers.layer.Layer:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the computations performed by the layer.\n",
            " |  \n",
            " |  dtype\n",
            " |      Alias of `layer.variable_dtype`.\n",
            " |  \n",
            " |  input_dtype\n",
            " |      The dtype layer inputs should be converted to.\n",
            " |  \n",
            " |  losses\n",
            " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
            " |  \n",
            " |  metrics\n",
            " |      List of all metrics.\n",
            " |  \n",
            " |  metrics_variables\n",
            " |      List of all metric variables.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |      List of all non-trainable layer state.\n",
            " |      \n",
            " |      This extends `layer.non_trainable_weights` to include all state used by\n",
            " |      the layer including state for metrics and `SeedGenerator`s.\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weight variables of the layer.\n",
            " |      \n",
            " |      These are the weights that should not be updated by the optimizer during\n",
            " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
            " |      state and random seeds.\n",
            " |  \n",
            " |  path\n",
            " |      The path of the layer.\n",
            " |      \n",
            " |      If the layer has not been built yet, it will be `None`.\n",
            " |  \n",
            " |  quantization_mode\n",
            " |      The quantization mode of this layer, `None` if not quantized.\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      List of all trainable layer state.\n",
            " |      \n",
            " |      This is equivalent to `layer.trainable_weights`.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weight variables of the layer.\n",
            " |      \n",
            " |      These are the weights that get updated by the optimizer during training.\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      The dtype of the state (weights) of the layer.\n",
            " |  \n",
            " |  variables\n",
            " |      List of all layer state, including random seeds.\n",
            " |      \n",
            " |      This extends `layer.weights` to include all state used by the layer\n",
            " |      including `SeedGenerator`s.\n",
            " |      \n",
            " |      Note that metrics variables are not included here, use\n",
            " |      `metrics_variables` to visit all the metric variables.\n",
            " |  \n",
            " |  weights\n",
            " |      List of all weight variables of the layer.\n",
            " |      \n",
            " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
            " |  \n",
            " |  dtype_policy\n",
            " |  \n",
            " |  input_spec\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |      Settable boolean, whether this layer should be trainable or not.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a symbolic operation.\n",
            " |      \n",
            " |      Only returns the tensor(s) corresponding to the *first time*\n",
            " |      the operation was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only returns the tensor(s) corresponding to the *first time*\n",
            " |      the operation was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output tensor or list of output tensors.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
            " |  \n",
            " |  __reduce__(self)\n",
            " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
            " |      \n",
            " |      The method returns a tuple of two elements: a function, and a list of\n",
            " |      arguments to pass to that function.  In this case we just leverage the\n",
            " |      keras saving library.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import LSTM\n",
        "help(LSTM)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
